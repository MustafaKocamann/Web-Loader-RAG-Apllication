{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y langchain langchain-community langchain-core langchain-groq\n",
        "!pip install langchain==0.1.20 langchain-community==0.0.38 langchain-core==0.1.52 langchain-groq==0.1.3 langchain-text-splitters cassio python-dotenv\n",
        "print(\"\\n‚úÖ Download complete!.\")"
      ],
      "metadata": {
        "id": "J6DwGBwGANpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import time\n",
        "\n",
        "print(\"The Ollama server is preparing...\")\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "print(\"The server is starting up...\")\n",
        "process = subprocess.Popen(\"ollama serve\", shell=True)\n",
        "time.sleep(5)\n",
        "\n",
        "print(\"Model is downloading (llama3.2:3b)...\")\n",
        "!ollama pull llama3.2:3b\n",
        "\n",
        "print(\"\\n‚úÖ OLLAMA is Ready!\")"
      ],
      "metadata": {
        "id": "twGCNb17Bl7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import bs4\n",
        "import cassio\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.embeddings import OllamaEmbeddings\n",
        "from langchain_community.vectorstores import Cassandra\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "\n",
        "\n",
        "GROQ_API_KEY = \"Your API Key\"\n",
        "\n",
        "ASTRA_DB_TOKEN = \"Your Astra DB Token\"\n",
        "ASTRA_DB_ID = \"Your Astra DB\"\n",
        "\n",
        "os.environ[\"USER_AGENT\"] = \"MyColabApp/1.0\"\n",
        "\n",
        "print(\"Connecting to the database...\")\n",
        "try:\n",
        "    cassio.init(token=ASTRA_DB_TOKEN, database_id=ASTRA_DB_ID)\n",
        "    print(\"‚úÖ Database Connection Successful!\")\n",
        "except Exception as e:\n",
        "    print(\"\\n‚ùå Database Error! Make sure the database is ‚ÄòActive‚Äô (Green) in the Astra DB panel.\")\n",
        "    print(\"Hata Detayƒ±:\", e)\n",
        "    raise e\n",
        "\n",
        "# EMBEDDING VE VECTOR DATABASE\n",
        "print(\"The Ollama model is being prepared...\")\n",
        "embeddings = OllamaEmbeddings(model=\"llama3.2:3b\")\n",
        "\n",
        "astra_vector_store = Cassandra(\n",
        "    embedding=embeddings,\n",
        "    table_name=\"qa_mini_demo_simple\",\n",
        "    session=None\n",
        ")\n",
        "\n",
        "#  DATA RETRIEVAL\n",
        "print(\"Web data is being retrieved...\")\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "    bs_kwargs=dict(parse_only=bs4.SoupStrainer(\n",
        "        class_=(\"post-title\", \"post-content\")\n",
        "    ))\n",
        ")\n",
        "text_documents = loader.load()\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "docs = text_splitter.split_documents(text_documents)\n",
        "\n",
        "\n",
        "# RAG CHAƒ∞NS\n",
        "print(\"Groq is being prepared...\")\n",
        "\n",
        "llm = ChatGroq(\n",
        "    groq_api_key=GROQ_API_KEY,\n",
        "    model_name=\"llama-3.3-70b-versatile\"\n",
        ")\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "Answer the question based ONLY on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {input}\n",
        "\"\"\")\n",
        "\n",
        "retriever = astra_vector_store.as_retriever()\n",
        "document_chain = create_stuff_documents_chain(llm, prompt)\n",
        "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
        "\n",
        "\n",
        "print(\"\\n--- SONU√á ---\")\n",
        "response = retrieval_chain.invoke({\"input\": \"What is Chain of Thought?\"})\n",
        "print(f\"\\nCEVAP: {response['answer']}\")"
      ],
      "metadata": {
        "id": "VImOp7U9CFyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- RAG CHAT BOT HAS STARTED (Type ‚Äòq‚Äô to exit) ---\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"\\Write your question: \")\n",
        "\n",
        "    if user_input.lower() == 'q':\n",
        "        print(\"See u!\")\n",
        "        break\n",
        "\n",
        "    # Soruyu zincire g√∂nder\n",
        "    response = retrieval_chain.invoke({\"input\": user_input})\n",
        "\n",
        "    print(f\"\\nü§ñ CEVAP: {response['answer']}\")\n",
        "    print(\"-\" * 30)"
      ],
      "metadata": {
        "id": "wCakzQEmF7_g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}